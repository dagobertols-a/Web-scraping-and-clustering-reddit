{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "#import contractions\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data = pd.concat(map(pd.read_csv, ['subreddits_data.csv']))\n",
    "reddit_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this dataset there are several columns of type string but they must be numerical.\n",
    "numeric_columns= ['score','comms_num','author_comments_karma', 'author_links_karma']\n",
    "for column in numeric_columns:\n",
    "    reddit_data[column]=pd.to_numeric(reddit_data[column], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column timestamp is of type string, let us convert it to type timestamp\n",
    "reddit_data['timestamp']=pd.to_datetime(reddit_data['timestamp']) \n",
    "# Number of reddits made in each hour of the day.\n",
    "reddit_data['reddit_created_hour']=reddit_data['timestamp'].apply(lambda time: time.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see the distribution of reddits for each hour.\n",
    "sns.catplot(x=\"reddit_created_hour\", kind=\"count\", data=reddit_data)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that most reddits are published between 3 p.m. and 7 p.m., which is not a surprise.\n",
    "Now, let us see the distribution of how much time has elapsed since each reddit was created(measured in days)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "#Time interval(measured in hours) since the reddits was created until now.\n",
    "reddit_data['elapsed_hours']=reddit_data.timestamp.\\\n",
    "apply(lambda timestamp: (datetime.datetime.now()-timestamp)/np.timedelta64(1,'h'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_percentile =np.percentile(reddit_data['elapsed_hours'],25)\n",
    "median =np.percentile(reddit_data['elapsed_hours'],50)\n",
    "third_percentile =np.percentile(reddit_data['elapsed_hours'],75)\n",
    "sns.distplot(reddit_data['elapsed_hours'], bins=20, kde=False, rug=False)\n",
    "\n",
    "plt.axvline(first_percentile, color=\"green\",linestyle=\"--\",label=\"25%_percentile\")\n",
    "plt.axvline(median, color=\"red\",linestyle=\"--\",label=\"median\")\n",
    "plt.axvline(third_percentile, color=\"green\",linestyle=\"--\",label=\"75%_percentile\")\n",
    "plt.title('Counting of elapsed hours since reddits creation')\n",
    "plt.xlabel('Elapsed_hours')\n",
    "plt.ylabel('Count');\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Min and Max hours elapsed since reddit creation, min: {:.0f}, max: {:.0f}.\"\\\n",
    "      .format(reddit_data['elapsed_hours'].min(),reddit_data['elapsed_hours'].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantities such as the score and the number of comments grow over time. Then, the proper way to use these features is to normalize them with respect to elapsed_hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para hacer el grafico de la frecuencia de los datos numericos primero normalizemos estas features\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# MinMaxScaler instance\n",
    "scaler = MinMaxScaler()\n",
    "#This estimator scales and translates each feature individually \n",
    "# From sklearn documentation, the transformation is calculated as follow:\n",
    "#X_scaled = scale * X + min - X.min(axis=0) * scale\n",
    "#where scale = (max - min) / (X.max(axis=0) - X.min(axis=0))\n",
    "# Numeric columns\n",
    "reddits_standardize =reddit_data[['score','comms_num','author_comments_karma', 'author_links_karma']]\n",
    "# Taking into account elapsed_hours\n",
    "reddits_standardize=reddits_standardize.div(reddit_data['elapsed_hours'],axis=0)\n",
    "\n",
    "# Remove null values\n",
    "reddits_standardize = reddits_standardize.dropna()\n",
    "# Fit and transform\n",
    "X_standardize =scaler.fit_transform(reddits_standardize)\n",
    "# Dataframe with numeric features\n",
    "data_standardize = pd.DataFrame(X_standardize,columns=['score','comms_num','author_comments_karma', \\\n",
    "                                                      'author_links_karma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_standardize.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In data_standardize, the values of each feature is normalized to the interval $[0,1]$. But, I am also\n",
    "interested in exploring the behavior of the logarithm of these columns.\n",
    "Since logarithm is undefined at zero, let's shift the min of each feature towards $1.0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifted_data_standardize =data_standardize+1.0\n",
    "log_data_standardize=np.log(shifted_data_standardize)\n",
    "log_data_standardize.columns=list(map(lambda x: 'log_'+x, data_standardize.columns))\n",
    "shifted_data_standardize.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that plots histograms of numerical columns.\n",
    "\n",
    "def plot_histogram(dataframe,column,y_label,title,bins=8):\n",
    "    '''\n",
    "    Take a column and plot two histograms, where:\n",
    "    left: Distribution of column values\n",
    "    right: Distribution of log of column values\n",
    "    \n",
    "    '''\n",
    "    # Params:\n",
    "    \n",
    "    # dataframe is the data frame to plot\n",
    "    # column is the numeric feature \n",
    "    # y_label is the y label \n",
    "    # title is the title of the plot\n",
    "    # bins is the number of bins\n",
    "     \n",
    "    \n",
    "    #Return : \n",
    "    \n",
    "    #Two, left: column values, right: logarithm of column values\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1,2,figsize=(8, 6),sharey=True)\n",
    "    fig.suptitle(title)\n",
    "    # Feature values \n",
    "    ax1.hist(dataframe[column],bins=bins,log=True)\n",
    "    #Log of feature values \n",
    "    ax2.hist(np.log(dataframe[column]),bins=bins,log=True)\n",
    "    # Labels\n",
    "    ax1.set_xlabel(column, labelpad=15, fontsize=12, color=\"black\");\n",
    "    ax1.set_ylabel(y_label, labelpad=15, fontsize=12, color=\"black\");\n",
    "    ax2.set_xlabel(\"Log of \"+ column, labelpad=15, fontsize=12, color=\"black\");\n",
    "    \n",
    "    #ax.title.set_text(title)\n",
    "    #ax.set_axisbelow(True)\n",
    "    plt.grid(True, alpha=0.5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms of the columns\n",
    "for column in  shifted_data_standardize.columns:\n",
    "    plot_histogram(shifted_data_standardize,column,\"Frequency (Log scale)\", \"Histogram of \" + column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphics above show very skewed distributions for all features. In fact, this behavior occurs in the plots of the logarithms too. Now, let us analyze the correlation among the  features(in log scale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data_standardize.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating correlation heatmap \n",
    "sns.heatmap(log_data_standardize.corr(), annot = True)  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have two goups of features. The first group is associated with the reddit author, these features are author_comments_karma\tand author_links_karma.The above map suggest a kind of correlation between them. \n",
    "The second group is associated with the reddit itself, these are score\tand comms_num, in this case there is a strong correlation between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression line for features log_score and log_comms_num\n",
    "g = sns.jointplot(x=log_data_standardize[\"log_score\"],\\\n",
    "                  y=log_data_standardize[\"log_comms_num\"],\\\n",
    "                  data=log_data_standardize,kind='reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I said before, log_score and  log_comms_num are strongly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression line for features log_links_karma\" and \"log_comments_karma\n",
    "g = sns.jointplot(x=log_data_standardize[\"log_author_comments_karma\"],\\\n",
    "                  y=log_data_standardize[\"log_author_links_karma\"],\\\n",
    "                  data=log_data_standardize,kind='reg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, log_author_comments_karma and log_author_links_karma show a weak correlation between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression line for features log_score  and log_author_links_karma\n",
    "g = sns.jointplot(x=log_data_standardize[\"log_score\"],\\\n",
    "                  y=log_data_standardize[\"log_author_links_karma\"],\\\n",
    "                  data=log_data_standardize,kind='reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the correlation is week too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see if a 3d plot allows us to detect any pattern between these features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(figsize=(12,10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "x= log_data_standardize[\"log_author_comments_karma\"]\n",
    "y=log_data_standardize[\"log_author_links_karma\"]\n",
    "z= log_data_standardize[\"log_score\"]\n",
    "ax.scatter(x, y, z, s=50)\n",
    "fig.suptitle('comments_karma vs links_karma vs score in log scale')\n",
    "\n",
    "ax.set_xlabel('log comments_karma')\n",
    "ax.set_ylabel('log links_karma')\n",
    "ax.set_zlabel('log score')\n",
    "\n",
    "plt.show()\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all points are grouped at the origin and there are several outliers.\n",
    "Let us see if reddits are organized by clusters.\n",
    "First, let us remove the superfluous features,so,  let us use a dimensional reduction technique such as\n",
    "principal component analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X_standardize=log_data_standardize.values\n",
    "\n",
    "# Create scaler: scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create PCA instance: pca\n",
    "pca = PCA()\n",
    "\n",
    "# Create pipeline: pipeline\n",
    "pipeline = make_pipeline(scaler,pca)\n",
    "\n",
    "#Fit and transform the pipeline to samples\n",
    "pipeline.fit_transform(X_standardize)\n",
    "\n",
    "# Plot percent of variance of each pca_feature\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') #for each component\n",
    "plt.title('Reddit features Explained Variance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous graph, almost $100$% of the variation is explained by $2$ characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PCA instance: pca\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Create pipeline: pipeline\n",
    "pipeline = make_pipeline(scaler,pca)\n",
    "pipeline.fit(X_standardize)\n",
    "# The pca features\n",
    "Xpca = pipeline.transform(X_standardize)\n",
    "\n",
    "# Plot of datapoints using pca_features\n",
    "sns.set()\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(Xpca[:,0],Xpca[:,1])\n",
    "plt.xlabel(\"PCA_feature_1\")\n",
    "plt.ylabel(\"PCA_feature_2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can see several outliers.\n",
    "Let us explore some clustering techniques to analyze if our data has clustering structure.\n",
    "Let us begin with the most popular one: K-Means. Essentially, K-Means sets k centroids in the data and clusters points by assigning them to the nearest centroid.  A way to select the optimal number of clusters is by using the elbow method. At high level, the idea is to find the least number of cluster where you can see  an elbow in the plot of number of clusters vs inertia(check this link https://www.scikit-yb.org/en/latest/api/cluster/elbow.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "range_n_clusters = range(2,15)\n",
    "inertias=[]\n",
    "\n",
    "for k in range_n_clusters: \n",
    "    #Building and fitting the model for the chosen number of clusters \n",
    "    clusterer_kmeans = KMeans(n_clusters=k)\n",
    "    cluster_labels = clusterer_kmeans.fit_predict(Xpca)\n",
    "    # Adding inertias to a list \n",
    "    inertias.append(clusterer_kmeans.inertia_) \n",
    "    \n",
    "# Number of clusters vs Inertias   \n",
    "plt.plot(range_n_clusters, inertias, 'bo-') \n",
    "plt.xlabel('Number of clusters') \n",
    "plt.ylabel('Inertia') \n",
    "plt.title('The Elbow Method using Inertia') \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elbow method suggests us that there are 2-6 clusters in the dataset.\n",
    "Let us explore the silhouette method for different clustering algorithms.\n",
    "The silhouette value determines how similar a given point  is to its own cluster compared to other clusters. The Silhouette values  are in the interval $[-1,1]$, where $1$ indicates that the point is well matched to its own cluster and poorly matched to neighboring clusters. If most pints  have a high value, then the clustering configuration is appropriate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering as AggClus\n",
    "from sklearn.cluster import SpectralClustering as SpectClus\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "#import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.cm as cm\n",
    "import re \n",
    "\n",
    "\n",
    "\n",
    "def silhouette_plots(range_n_clusters,X,method=KMeans):\n",
    "    # Params:\n",
    "    # range_n_clusters is a list containing the different cluster numbers that we are going to analyze.\n",
    "    # X is the  data\n",
    "    # method is the clustering method\n",
    "    \n",
    "    # Return:\n",
    "    # Two plots, for each number of clusters, left plot: silhouette_score \n",
    "    #and right plot: is the scatter plot of our features \n",
    "    \n",
    "    for n_clusters in range_n_clusters:\n",
    "        # Create a subplot with 1 row and 2 columns\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        fig.set_size_inches(14, 6)\n",
    "\n",
    "        # The 1st subplot is the silhouette plot\n",
    "        # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "        # lie within [-0.4, 1]\n",
    "        ax1.set_xlim([-0.6, 1])\n",
    "        # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "        # plots of individual clusters, to demarcate them clearly.\n",
    "        ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "        # Initialize the clusterer with n_clusters value \n",
    "        clusterer = method(n_clusters=n_clusters)\n",
    "        cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "        # The silhouette_score gives the average value for all the samples.\n",
    "        # This gives a perspective into the density and separation of the formed\n",
    "        # clusters\n",
    "        silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "        print(\"For n_clusters =\", n_clusters,\n",
    "              \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "        # Compute the silhouette scores for each sample\n",
    "        sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "        \n",
    "\n",
    "        y_lower = 10\n",
    "        for i in range(n_clusters):\n",
    "            # Aggregate the silhouette scores for samples belonging to\n",
    "            # cluster i, and sort them\n",
    "            ith_cluster_silhouette_values = \\\n",
    "                sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "            ith_cluster_silhouette_values.sort()\n",
    "\n",
    "            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "            y_upper = y_lower + size_cluster_i\n",
    "\n",
    "            color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "            ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                              0, ith_cluster_silhouette_values,\n",
    "                              facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "            # Label the silhouette plots with their cluster numbers at the middle\n",
    "            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "            # Compute the new y_lower for next plot\n",
    "            y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "        ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "        ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "        ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "        # The vertical line for average silhouette score of all the values\n",
    "        ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "        ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "        ax1.set_xticks([-0.6,-0.4,-0.2, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "        # 2nd Plot showing the actual clusters formed\n",
    "        colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "        ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                    c=colors, edgecolor='k')\n",
    "\n",
    "        # Labeling the clusters\n",
    "        if method == KMeans:\n",
    "            centers = clusterer.cluster_centers_\n",
    "            #Draw white circles at cluster centers\n",
    "            ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\\\n",
    "                        c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "            for i, c in enumerate(centers):\n",
    "                ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\\\n",
    "                            s=50, edgecolor='k')\n",
    "\n",
    "        ax2.set_title(\"The visualization of the clustered data.\")\n",
    "        ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "        ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "        # Get the name of the clustering method\n",
    "        method_name=str(method().__class__)\n",
    "        # Extract only the name \n",
    "        str_method= re.findall(r\".[\\w]+'\", method_name)[0][1:-1]\n",
    "        \n",
    "        plt.suptitle((\"Silhouette analysis for \" + str_method + \" on sample data with n_clusters = %d\" % n_clusters),\n",
    "                     fontsize=14, fontweight='bold')\n",
    "        \n",
    "        \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_n_clusters = [2,3,4,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_plots(range_n_clusters,Xpca,KMeans);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "silhouette_plots(range_n_clusters,Xpca,SpectClus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_plots(range_n_clusters,Xpca,AggClus) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using various clustering methods, the silhouette score suggests that the optimal number of clusters is 2.\n",
    "Let us study  density-based clustering, this kind of algorithms  are good identifying data points that deviate from the normal distribution, in other words, outliers. A popular density based method is DBSCAN. This method  does not require the number of clusters as a parameter. Here, the main parameters are the radius of the neighborhoods and the minimum number points you want in a neighborhood to define a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "# Object dbscan\n",
    "dbsc = DBSCAN(eps = .8, min_samples = 10)\n",
    "# fit and transform\n",
    "labels_dbsc = dbsc.fit_predict(Xpca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Xpca[:,0],Xpca[:,1],c=labels_dbsc);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how these two clusters look in our original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "x= log_data_standardize[\"log_author_comments_karma\"]\n",
    "y=log_data_standardize[\"log_author_links_karma\"]\n",
    "z= log_data_standardize[\"log_score\"]\n",
    "ax.scatter(x, y, z, s=50,c=labels_dbsc)\n",
    "fig.suptitle('Two clusters: comments_karma vs links_karma vs score in log scale')\n",
    "\n",
    "ax.set_xlabel('log comments_karma')\n",
    "ax.set_ylabel('log links_karma')\n",
    "ax.set_zlabel('log score')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see what are the most used words in  reddits that are outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reddits_not_null=reddit_data[~reddit_data.author_comments_karma.isnull()]\n",
    "# New column with cluster labels\n",
    "reddits_not_null['is_outliers'] =labels_dbsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddits_not_null['is_outliers'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 32 outliers in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers reddits \n",
    "reddits_not_null[reddits_not_null['is_outliers']==-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before exploring the most common words used in the reddits for each cluster, let us  normalize the text of the titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(words):\n",
    "    #Params: words is the words in title\n",
    "    \n",
    "    #Return: normalized list of words\n",
    "    \n",
    "    list_words = []\n",
    "    #lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for word in words:\n",
    "        #Remove non-ASCII characters from the word\n",
    "        word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        # Convert all characters to lowercase\n",
    "        word = word.lower()\n",
    "        #Removing punctuation\n",
    "        word = re.sub(r'[^A-Za-z\\s]', '', word)\n",
    "        #Removing stop words\n",
    "        if word != '' and  word not in stopwords.words('english'):\n",
    "            #word = stemmer.stem(word)\n",
    "            # Eliminating short words and maintaining negations\n",
    "            if (word !='no' or word !='nt')  and len(word)>2:\n",
    "                word = lemmatizer.lemmatize(word, pos='v')\n",
    "            \n",
    "            list_words.append(word)\n",
    "    return list_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words tokenizer\n",
    "tokenized_title=reddits_not_null.title.apply(lambda entry: nltk.word_tokenize(entry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization of titles\n",
    "clean_titles=tokenized_title.apply(lambda entry: normalization(entry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming each entry of clean_titles from type list to string.\n",
    "reddits_not_null['clean_title']=clean_titles.apply(lambda x: \" \".join(x))\n",
    "reddits_not_null.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most used words for outliers \n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "word_cloud = WordCloud(stopwords=STOPWORDS)\n",
    "plt.figure(figsize=(14, 14))\n",
    "word_cloud.generate(str(reddits_not_null[reddits_not_null['is_outliers']==-1].clean_title))\n",
    "plt.imshow(word_cloud)\n",
    "plt.title('Most common words for outliers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most used words for non outliers \n",
    "word_cloud = WordCloud(stopwords=STOPWORDS)\n",
    "plt.figure(figsize=(14, 14))\n",
    "word_cloud.generate(str(reddits_not_null[reddits_not_null['is_outliers']==0].clean_title))\n",
    "plt.imshow(word_cloud)\n",
    "plt.title('Most common words for outliers')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
